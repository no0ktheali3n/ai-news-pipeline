{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "669e26d6-e632-4784-a284-e69d0f9f613f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üßµ === Thread for Article 1 ===\n",
      "\n",
      "--- Tweet 1 ---\n",
      "Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual Learning\n",
      "üß†‚ú® Exciting AI breakthrough alert! üöÄ Scientists have cracked the code on making language models learn new tricks without forgetting old ones. It's like teaching an old dog new tricks, but the dog remembers ALL its tricks!\n",
      "Characters: 302\n",
      "\n",
      "--- Tweet 2 ---\n",
      "üê∂üéì\n",
      "\n",
      "This clever method, using some fancy math called \"adaptive SVD,\" lets AI models keep learning without getting brain freeze. üßäü§Ø The result? AI that's smarter, more flexible, and doesn't need a gazillion extra parts to keep growing. Now that's what we call a win-win! üèÜüéâ\n",
      "Characters: 272\n",
      "\n",
      "--- Tweet 3 ---\n",
      "https://arxiv.org/abs/2504.07097\n",
      "#AI #ContinualLearning #LLM #CatastrophicForgetting\n",
      "Characters: 84\n",
      "\n",
      "üßµ === Thread for Article 2 ===\n",
      "\n",
      "--- Tweet 1 ---\n",
      "Are We Done with Object-Centric Learning?\n",
      "ü§ñüì∏ Object-centric learning just got a major upgrade! Researchers have found a way to separate objects in images more efficiently than ever before.\n",
      "Characters: 188\n",
      "\n",
      "--- Tweet 2 ---\n",
      "üéâ\n",
      "\n",
      "Instead of complex unsupervised methods, they're using cutting-edge segmentation models to isolate objects right in the pixel space.\n",
      "Characters: 135\n",
      "\n",
      "--- Tweet 3 ---\n",
      "üß©‚ú®\n",
      "\n",
      "This breakthrough could revolutionize how AI understands and generalizes visual information, paving the way for more robust and adaptable computer vision systems. üöÄüî¨ #AIResearch #ComputerVision\n",
      "Characters: 197\n",
      "\n",
      "--- Tweet 4 ---\n",
      "https://arxiv.org/abs/2504.07092\n",
      "#AI #ObjectCentricLearning #ComputerVision #AIRepresentation\n",
      "Characters: 93\n",
      "\n",
      "‚úÖ Exported 7 tweets across 2 threads to:\n",
      "- C:\\Users\\hano0\\Desktop\\github projects\\scraper-alpha\\tweet_threads.json\n",
      "- C:\\Users\\hano0\\Desktop\\github projects\\scraper-alpha\\tweet_threads.csv\n"
     ]
    }
   ],
   "source": [
    "# poster-preview.ipynb (Tweet formatter notebook with Claude + static + dynamic hashtags)\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import textwrap\n",
    "import csv\n",
    "\n",
    "# üß≠ Robust project root resolver\n",
    "try:\n",
    "    PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "except NameError:\n",
    "    PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "SUMMARY_PATH = os.path.join(PROJECT_ROOT, \"summarized_output.json\")\n",
    "EXPORT_JSON = os.path.join(PROJECT_ROOT, \"tweet_threads.json\")\n",
    "EXPORT_CSV = os.path.join(PROJECT_ROOT, \"tweet_threads.csv\")\n",
    "\n",
    "DEFAULT_HASHTAGS = [\"#AI\"]\n",
    "MAX_TWEET_LENGTH = 280\n",
    "\n",
    "# Load summaries\n",
    "try:\n",
    "    with open(SUMMARY_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        articles = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(f\"[ERROR] summarized_output.json not found at: {SUMMARY_PATH}\")\n",
    "    articles = []\n",
    "\n",
    "# Sentence splitting regex\n",
    "def split_sentences(text):\n",
    "    return re.split(r'(?<=[.!?]) +', text.strip())\n",
    "\n",
    "# Chunk summary into tweetable parts\n",
    "def chunk_summary_into_thread(summary, title, url, hashtags=DEFAULT_HASHTAGS):\n",
    "    sentences = split_sentences(summary)\n",
    "    thread = []\n",
    "    current = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(current) + len(sentence) + 1 <= MAX_TWEET_LENGTH:\n",
    "            current += (\" \" if current else \"\") + sentence\n",
    "        else:\n",
    "            thread.append(current.strip())\n",
    "            current = sentence\n",
    "    if current:\n",
    "        thread.append(current.strip())\n",
    "\n",
    "    # Add title to first tweet\n",
    "    if title:\n",
    "        thread[0] = f\"{title}\\n{thread[0]}\"\n",
    "\n",
    "    # Final tweet: link + hashtags\n",
    "    hashtag_block = \" \".join(hashtags)\n",
    "    link_tweet = f\"{url}\\n{hashtag_block}\".strip()\n",
    "    if len(link_tweet) > MAX_TWEET_LENGTH:\n",
    "        hashtag_block = \"#AI\"\n",
    "        link_tweet = f\"{url}\\n{hashtag_block}\"\n",
    "    thread.append(link_tweet)\n",
    "\n",
    "    return thread\n",
    "\n",
    "# Format + export\n",
    "all_threads = []\n",
    "\n",
    "for i, article in enumerate(articles[:5]):\n",
    "    title = article.get(\"title\", \"\")\n",
    "    summary = article.get(\"v1_summary\", \"\")\n",
    "    url = article.get(\"url\", \"\")\n",
    "    dyn_tags = [tag for tag in article.get(\"hashtags\", \"\").split() if tag.startswith(\"#\")]\n",
    "    hashtags = list(dict.fromkeys(DEFAULT_HASHTAGS + dyn_tags[:3]))\n",
    "\n",
    "    thread = chunk_summary_into_thread(summary, title, url, hashtags)\n",
    "\n",
    "    for j, tweet in enumerate(thread):\n",
    "        all_threads.append({\n",
    "            \"article_index\": i + 1,\n",
    "            \"thread_index\": j + 1,\n",
    "            \"tweet\": tweet,\n",
    "            \"characters\": len(tweet),\n",
    "            \"url\": url,\n",
    "            \"hashtags\": \" \".join(hashtags)\n",
    "        })\n",
    "\n",
    "    print(f\"\\nüßµ === Thread for Article {i+1} ===\")\n",
    "    for j, tweet in enumerate(thread):\n",
    "        print(f\"\\n--- Tweet {j+1} ---\\n{tweet}\\nCharacters: {len(tweet)}\")\n",
    "\n",
    "# Save outputs\n",
    "with open(EXPORT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_threads, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(EXPORT_CSV, \"w\", encoding=\"utf-8\", newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=all_threads[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_threads)\n",
    "\n",
    "print(f\"\\n‚úÖ Exported {len(all_threads)} tweets across {len(articles)} threads to:\")\n",
    "print(f\"- {EXPORT_JSON}\")\n",
    "print(f\"- {EXPORT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae021889-98de-4658-af2e-b80916de4043",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
