# AI News Poster Pipeline

A serverless automation tool that scrapes AI-related news, summarizes it using large language models (LLMs), and automatically posts it to Twitter/X. Designed with modular AWS Lambda functions, local notebooks, and will eventually integrate scheduled events (v0.6.0) and GitHub CI/CD pipeline (0.7.0+).

This project is open source for educational and non-commercial use.  
For commercial licensing, please contact me directly.

---

## Tech Stack

- **AWS Lambda** â€“ Serverless compute for scraper, summarizer, and poster
- **Amazon Bedrock** â€“ LLM summarization (Claude 3.5 Sonnet)
- **Pandas** â€“ Tabular data transformation for summarizer output
- **Jupyter Notebooks** â€“ Local testing + LLM prompt tuning
- **Tweepy** â€“ Social media automation (future)
- **GitHub Actions** â€“ CI/CD deployment (planned)
- **AWS SAM** â€“ Infrastructure as Code for packaging/deploying Lambda
- **CloudWatch** â€“ Logging and monitoring
- **Rotating User Agents** â€“ Basic stealth and rate-limit avoidance for scraping

---

## Architecture

```
EventBridge (Scheduled Trigger)
       |
       v
AWS Lambda [Scraper] --> Bedrock Claude [Summarizer] --> AWS Lambda [Poster] --> Twitter API

[CI/CD Pipeline]:
GitHub --> GitHub Actions --> SAM Deploy --> CloudWatch Logs
```

---

## IAM Configuration

This project uses Amazon Bedrock and other AWS services that require explicit permissions.

### IAM Policy Example

```
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "bedrock:InvokeModel",
        "bedrock:ListFoundationModels"
      ],
      "Resource": "*"
    }
  ]
}
```

### Usage
- **For AWS Lambda**: Attach policy to your functionâ€™s execution role.
- **For local testing**: Create an IAM User with programmatic access and configure using `.env`.

---

## Project Structure

```
(requirements loaded per each specific function to avoid dependency bloat and lambda size limitations - 262mb)
ai-news-poster-pipeline/
â”œâ”€â”€ lambda/
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â”œâ”€â”€ pipeline_lambda.py         # Lambda entrypoint â€“ runs full scrape, summary, post process
â”‚   â”œâ”€â”€ scraper/
â”‚   â”‚   â”œâ”€â”€ scraper_lambda.py          # Lambda entrypoint â€“ pulls articles from ArXiv
â”‚   â”‚   â”œâ”€â”€ requirements.txt           # Local deps if needed (e.g., bs4, pandas)
â”‚   â”œâ”€â”€ summarizer/
â”‚   â”‚   â”œâ”€â”€ summarizer_lambda.py       # summarizes via Claude
|   |   â”œâ”€â”€ summarizer_main_lambda.py  # Lambda entrypoint - chunks data and summarizes via summarizer_lambda
â”‚   â”‚   â”œâ”€â”€ requirements.txt           # summarizer requirements
â”‚   â”œâ”€â”€ poster/
â”‚   â”‚   â”œâ”€â”€ poster_lambda.py           # Lambda entrypoint â€“ posts tweet threads
â”‚   â”‚   â”œâ”€â”€ requirements.txt           # Only tweepy + formatters
â”‚
â”‚   â””â”€â”€ layers/
â”‚       â””â”€â”€ common/
â”‚           â””â”€â”€ python/
â”‚               â””â”€â”€ utils/
â”‚                   â”œâ”€â”€ __init__.py               # Required for Python to treat as a package
                    â”œâ”€â”€ chunker.py                # chunks scraped articles for summary according to chunk_size
                    â”œâ”€â”€ logger.py                 # Standardized logger for all Lambda functions
                    â”œâ”€â”€ memcon.py                 # memory controller for scraper and future poster functionality
                    â”œâ”€â”€ post_to_twitter.py        # Main orchestrator for tweet threading
                    â”œâ”€â”€ request_helpers.py        # Delays, header modifiers, request pacing
                    â”œâ”€â”€ scraper.py                # ArXiv-specific scrape logic
                    â”œâ”€â”€ summarizer.py             # Claude summarization + hashtag prompt logic
                    â”œâ”€â”€ test_tweet.py             # Standalone testing for tweet formatting
                    â”œâ”€â”€ tweepy_client.py          # Twitter API client with auth
                    â”œâ”€â”€ twitter_threading.py      # Splits summary into threaded tweets
                    â””â”€â”€ user_agents.py            # Randomized browser headers
â”‚
â”œâ”€â”€ utils/                           # [Legacy] local utils or notebooks support
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ summarization-lab.ipynb      # Claude tuning, rate limit handling, retries
â”‚   â”œâ”€â”€ poster-lab.ipynb             # Thread preview tool with validator
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ README.md                    # Full pipeline overview, goals, and usage
â”‚   â”œâ”€â”€ CHANGELOG.md                 # Incremental feature tracking (e.g. v0.4.3 â†’ v0.5.0)
â”‚   â”œâ”€â”€ TECHNICAL_DIARY.md          # Live journal of issues, fixes, decisions
â”‚   â””â”€â”€ SECURITY.md                 # Secret mgmt + IAM handling practices
â”‚
â”œâ”€â”€ iam/
â”‚   â”œâ”€â”€ admin-policy.json            # Custom admin IAM policy for full access
â”‚   â””â”€â”€ user-policy.json             # Constrained IAM policy for Lambda runners
â”‚
â”œâ”€â”€ test_output.json                # Cached scraper test run (manual / local test)
â”œâ”€â”€ summarized_output.json          # Local summarizer output (Claude v1 + hashtags)
â”œâ”€â”€ poster_event.json               # Sample event to test Twitter poster logic
â”‚
â”œâ”€â”€ .env.example                    # Safe reference for AWS + Claude vars
â”œâ”€â”€ .gitignore                      # Prevents .env, .aws-sam, outputs from leaking
â”œâ”€â”€ requirements.txt                # Dev-wide package list (env-agnostic)
â”œâ”€â”€ sam-template.yaml               # SAM template w/ layer ref and env injection
â”œâ”€â”€ samconfig.toml                  # SAM CLI config (region, stack, param overrides)
â””â”€â”€ venv/                           # Python virtualenv (ignored by git)
```

---

## Notebooks

| Notebook | Purpose |
|----------|---------|
| `summarization-lab.ipynb` | Claude (via Bedrock) summarization previewer with token usage and retry logic |
| `poster-lab.ipynb` | Auto-formats tweet-ready summaries with hashtags, character limit enforcement |

---

## How to Use

### 1. Clone the repo

```
git clone https://github.com/no0ktheali3n/scraper-alpha.git
cd scraper-alpha
```

### 2. Set up your environment

```
cp .env.example .env
# Fill in AWS keys, model ID, region, etc.
```

### 3. Install dependencies

```
python -m venv venv
venv\Scripts\activate  # Windows
pip install -r docs\requirements.txt
```

### 4. Run locally

```
python lambda/scraper.py
jupyter notebook notebooks/summarization-lab.ipynb
```

### 5. (Optional) Deploy to AWS Lambda

```
sam build && sam deploy --guided
```

---

## GitHub Actions (CI/CD)

A sample workflow is under development to:
- Lint and test code
- Package and deploy functions using SAM
- Notify on success/failure

---

## Licensing & Attribution

### License
**MIT License**

### Citation Policy
This project summarizes publicly available research, primarily from [arXiv.org](https://arxiv.org/).  
We do not claim ownership. All posts contain backlink attribution to the original article.

### Ethics Disclaimer
- ğŸ” Cite original authors when sharing AI summaries  
- ğŸš« Do not present auto-generated outputs as original research  
- âœ… Use for learning, journalism, outreach, and educational content

---

## Contributor Note

Check the `/docs/` folder for:
- âœ… Full `CHANGELOG.md` with version history
- âœ… `SECURITY.md` with key management and Git history scrub
- âœ… `TECHNICAL_DIARY.md` â€” a log of decisions, ideas, and debugging sessions

---

**Want to contribute or fork this?**  
Submit an issue or open a pull request â€” improvements are welcome!
