{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce2dc4b3-38eb-47a5-8cbb-069096930a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anthropic.claude-3-5-sonnet-20240620-v1:0\n",
      "[2025-04-10T21:02:03.845477] Throttled. Retrying in 5.50s...\n",
      "[2025-04-10T21:02:18.205030] Throttled. Retrying in 8.77s...\n",
      "[2025-04-10T21:02:39.432538] Throttled. Retrying in 4.92s...\n",
      "[2025-04-10T21:02:48.668960] Throttled. Retrying in 7.68s...\n",
      "[2025-04-10T21:03:09.106302] Throttled. Retrying in 14.86s...\n",
      "[2025-04-10T21:03:43.212564] Throttled. Retrying in 4.80s...\n",
      "[2025-04-10T21:03:56.224967] Throttled. Retrying in 8.42s...\n",
      "[2025-04-10T21:04:14.361248] Throttled. Retrying in 13.54s...\n",
      "[2025-04-10T21:04:38.862744] Throttled. Retrying in 4.11s...\n",
      "[2025-04-10T21:04:53.816438] Throttled. Retrying in 8.38s...\n",
      "[2025-04-10T21:05:11.467889] Throttled. Retrying in 14.25s...\n",
      "[2025-04-10T21:05:50.354395] Throttled. Retrying in 5.68s...\n",
      "[2025-04-10T21:06:04.478523] Throttled. Retrying in 8.73s...\n",
      "[2025-04-10T21:06:20.628287] Throttled. Retrying in 13.12s...\n",
      "[2025-04-10T21:06:48.952897] Throttled. Retrying in 4.97s...\n",
      "[2025-04-10T21:07:03.060861] Throttled. Retrying in 7.07s...\n",
      "[2025-04-10T21:07:20.006471] Throttled. Retrying in 14.66s...\n",
      "--- Article 1 ---\n",
      "Prompt v1 Summary:\n",
      " üì∏‚ú® Ever wished your photos could talk? Meet URECA, the AI that's giving image regions a voice! üó®Ô∏è This clever system can describe any part of a picture, from the tiniest detail to the big picture. It's like having a chatty art critic in your pocket, ready to spill the tea on every brushstroke. üé® Who knows, maybe your selfies will finally get the poetic justice they deserve! #AIArtistry #TalkingPictures\n",
      "\n",
      "Prompt v2 Summary:\n",
      " This research paper introduces URECA, a novel approach to region-level image captioning that addresses limitations in existing methods, particularly the struggle to produce unique captions across multiple levels of detail (multi-granularity).\n",
      "\n",
      "Key elements of the paper include:\n",
      "\n",
      "1. URECA Dataset: A new large-scale dataset specifically designed for multi-granularity region captioning. Unlike previous datasets that focus mainly on prominent objects, URECA includes a diverse range of objects, parts, and background elements.\n",
      "\n",
      "2. Data Curation Pipeline: A stage-wise process that uses Multimodal Large Language Models (MLLMs) to refine region selection and caption generation, resulting in more distinctive and contextually relevant captions.\n",
      "\n",
      "3. URECA Model: A captioning model designed to effectively encode multi-granularity regions while preserving spatial properties like position and shape. This is achieved through modifications to existing MLLMs.\n",
      "\n",
      "4. Novel Techniques: The model incorporates dynamic mask modeling and a high-resolution mask encoder to improve caption uniqueness.\n",
      "\n",
      "5. Performance: URECA achieves state-of-the-art performance on its namesake dataset and demonstrates good generalization to existing region-level captioning benchmarks.\n",
      "\n",
      "The research is particularly relevant for advancing image understanding and description technologies, with potential applications in areas such as assistive technologies for the visually impaired, content indexing, and automated image analysis\n",
      "\n",
      "Suggested Hashtags:\n",
      " Here are 3-5 short and relevant hashtags for this AI paper:\n",
      "\n",
      "#RegionCaptioning\n",
      "#ComputerVision\n",
      "#MultimodalAI\n",
      "#ImageUnderstanding\n",
      "#URECA\n",
      "\n",
      "These hashtags cover the key aspects of the paper, including the specific task (region captioning), the broader field (computer vision), the use of multimodal AI, the focus on image understanding, and the name of the introduced dataset and model (URECA).\n",
      "\n",
      "\n",
      "--- Article 2 ---\n",
      "Prompt v1 Summary:\n",
      " üì±ü§ñ Honey, I shrunk the AI! Meet SmolVLM, the tiny but mighty vision-language model that packs a big punch in a small package. ü•äüí™\n",
      "\n",
      "This little genius outperforms models 300 times its size, using less than 1GB of memory - perfect for your smartphone or smart fridge! üß†üöÄ\n",
      "\n",
      "Who says good things only come in big packages? SmolVLM is proving that with clever design and the right data diet, AI can be lean, mean, and energy-efficient! üåøüí° #AIonADiet #SmallButSmort\n",
      "\n",
      "Prompt v2 Summary:\n",
      " Key points from the research paper:\n",
      "\n",
      "1. Novel approach: SmolVLM, a series of compact multimodal models designed for resource-efficient inference on mobile and edge devices.\n",
      "\n",
      "2. Methodology:\n",
      "   - Systematic exploration of architectural configurations\n",
      "   - Optimized tokenization strategies\n",
      "   - Careful data curation for low computational overhead\n",
      "\n",
      "3. Key findings:\n",
      "   - SmolVLM-256M (smallest model) outperforms Idefics-80B, a model 300 times larger, while using less than 1GB GPU memory during inference\n",
      "   - SmolVLM-2.2B (largest model) rivals state-of-the-art VLMs while consuming half the GPU memory\n",
      "   - Models demonstrate robust video comprehension capabilities\n",
      "\n",
      "4. Real-world relevance:\n",
      "   - Enables practical deployment of Vision-Language Models on mobile and edge devices\n",
      "   - Significantly reduces computational resources required for inference\n",
      "   - Extends capabilities beyond static images to include video comprehension\n",
      "\n",
      "5. Implications:\n",
      "   - Strategic architectural optimizations can greatly enhance multimodal performance\n",
      "   - Aggressive yet efficient tokenization is crucial for model efficiency\n",
      "   - Carefully curated training data plays a significant role in model performance\n",
      "\n",
      "In summary, this research presents a novel approach to creating compact, efficient Vision-Language Models that maintain high performance while significantly reducing computational requirements,\n",
      "\n",
      "Suggested Hashtags:\n",
      " Here are 3-5 short and relevant hashtags for this AI paper:\n",
      "\n",
      "1. #SmolVLM\n",
      "2. #EfficientAI\n",
      "3. #MultimodalML\n",
      "4. #CompactVLM\n",
      "5. #EdgeAI\n",
      "\n",
      "These hashtags capture the key aspects of the paper, including the model name (SmolVLM), its focus on efficiency and compact design, its multimodal nature, and its potential for edge device deployment.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# summarization-lab.ipynb (Claude + prompt variants + hashtags + retry + autosave)\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "aws_access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "aws_region = os.getenv(\"AWS_REGION\", \"us-east-1\")\n",
    "model_id = os.getenv(\"BEDROCK_MODEL_ID\", \"anthropic.claude-3-sonnet-20240229\")\n",
    "print(model_id)\n",
    "\n",
    "bedrock = boto3.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=aws_region,\n",
    "    aws_access_key_id=aws_access_key,\n",
    "    aws_secret_access_key=aws_secret_key\n",
    ")\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), '..'))\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT, 'test_output.json')\n",
    "SAVE_PATH = os.path.join(PROJECT_ROOT, 'summarized_output.json')\n",
    "\n",
    "# Load articles\n",
    "try:\n",
    "    with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        articles = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Could not find test_output.json at: {DATA_PATH}\")\n",
    "    articles = []\n",
    "\n",
    "# Prompt builders\n",
    "\n",
    "def build_prompt_v1(article):\n",
    "    return (\n",
    "        f\"Summarize the following AI research article in a fun and engaging way appropriate for social media using 3-4 sentences.\\n\"\n",
    "        f\"Title: {article['title']}\\n\"\n",
    "        f\"Authors: {', '.join(article['authors'])}\\n\"\n",
    "        f\"Abstract: {article['snippet']}\"\n",
    "    )\n",
    "\n",
    "def build_prompt_v2(article):\n",
    "    return (\n",
    "        f\"You are an expert technical writer. Provide a concise, informative summary of this research paper.\\n\"\n",
    "        f\"Focus on any novel methods, key findings, or real-world relevance.\\n\\n\"\n",
    "        f\"Abstract: {article['snippet']}\"\n",
    "    )\n",
    "\n",
    "def build_hashtag_prompt(article):\n",
    "    return (\n",
    "        f\"Suggest 3-5 short and relevant hashtags for this AI paper.\\n\"\n",
    "        f\"Title: {article['title']}\\n\"\n",
    "        f\"Abstract: {article['snippet']}\"\n",
    "    )\n",
    "\n",
    "# Claude summary function\n",
    "\n",
    "def summarize_with_claude(prompt):\n",
    "    payload = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 300,\n",
    "        \"temperature\": 0.7,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    }\n",
    "    response = bedrock.invoke_model(\n",
    "        modelId=model_id,\n",
    "        contentType=\"application/json\",\n",
    "        accept=\"application/json\",\n",
    "        body=json.dumps(payload)\n",
    "    )\n",
    "    result = json.loads(response[\"body\"].read())\n",
    "    return result[\"content\"][0][\"text\"].strip()\n",
    "\n",
    "# Retry logic with timeout\n",
    "\n",
    "def retry_until_timeout(func, max_seconds=600, base_delay=3):\n",
    "    start_time = time.time()\n",
    "    attempt = 0\n",
    "    while time.time() - start_time < max_seconds:\n",
    "        try:\n",
    "            return func()\n",
    "        except Exception as e:\n",
    "            if \"ThrottlingException\" in str(e):\n",
    "                delay = min(base_delay * (2 ** attempt), 60) + random.uniform(1, 3)\n",
    "                print(f\"[{datetime.utcnow().isoformat()}] Throttled. Retrying in {delay:.2f}s...\")\n",
    "                time.sleep(delay)\n",
    "                attempt += 1\n",
    "            else:\n",
    "                print(f\"[{datetime.utcnow().isoformat()}] Non-throttling error:\", e)\n",
    "                return \"[Summary unavailable]\"\n",
    "    return \"[Summary unavailable after max retry time]\"\n",
    "\n",
    "# Summarize and generate hashtags for each article\n",
    "\n",
    "summarized = []\n",
    "for article in articles[:2]:\n",
    "    p1 = build_prompt_v1(article)\n",
    "    p2 = build_prompt_v2(article)\n",
    "    htag_prompt = build_hashtag_prompt(article)\n",
    "\n",
    "    summary_v1 = retry_until_timeout(lambda: summarize_with_claude(p1))\n",
    "    summary_v2 = retry_until_timeout(lambda: summarize_with_claude(p2))\n",
    "    hashtags = retry_until_timeout(lambda: summarize_with_claude(htag_prompt))\n",
    "\n",
    "    result = {\n",
    "        **article,\n",
    "        \"v1_summary\": summary_v1,\n",
    "        \"v2_summary\": summary_v2,\n",
    "        \"hashtags\": hashtags\n",
    "    }\n",
    "    summarized.append(result)\n",
    "\n",
    "    with open(SAVE_PATH, \"w\", encoding=\"utf-8\") as out:\n",
    "        json.dump(summarized, out, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Preview result\n",
    "for i, entry in enumerate(summarized):\n",
    "    print(f\"--- Article {i+1} ---\")\n",
    "    print(\"Prompt v1 Summary:\\n\", entry['v1_summary'])\n",
    "    print(\"\\nPrompt v2 Summary:\\n\", entry['v2_summary'])\n",
    "    print(\"\\nSuggested Hashtags:\\n\", entry['hashtags'])\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb1012b-0681-441a-acc7-ff30724b5617",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
